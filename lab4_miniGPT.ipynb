{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ae06f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca8f041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd741fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка текста и токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a62697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpt_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = ['', ''] + sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi.get(c, stoi['']) for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "encoded_data = encode(\" \" + text + \" \")\n",
    "data = torch.tensor(encoded_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "744ea96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd00ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "batch_size = 32\n",
    "embedding_dim = 128\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "ff_hidden = 256\n",
    "n_epochs = 80\n",
    "dropout_rate = 0.1\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d06d07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4db490b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "dataloader = DataLoader(CharDataset(data, block_size), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98484ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention и Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0eebc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        att = att.masked_fill(torch.tril(torch.ones(T, T, device=att.device)) == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "483a6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden, embed_dim)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a065db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniGPT модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0106fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, n_heads, n_layers, ff_hidden):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, n_heads, ff_hidden)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))[None, :, :]\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c658912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f6ac4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.3865\n",
      "Epoch 10, Loss: 2.2122\n",
      "Epoch 20, Loss: 1.8076\n",
      "Epoch 30, Loss: 1.4425\n",
      "Epoch 40, Loss: 1.2491\n",
      "Epoch 50, Loss: 0.9015\n",
      "Epoch 60, Loss: 0.6900\n",
      "Epoch 70, Loss: 0.5465\n",
      "Epoch 79, Loss: 0.4254\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MiniGPT(vocab_size, embedding_dim, block_size, n_heads, n_layers, ff_hidden).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B * T, C), y.view(B * T))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3014ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d0bea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_text=\"Привет\", max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    context = torch.tensor(encode(start_text), dtype=torch.long)[None, :].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context_condensed = context[:, -block_size:]\n",
    "        logits = model(context_condensed)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if top_k is not None:\n",
    "            values, indices = torch.topk(probs, top_k)\n",
    "            probs = torch.zeros_like(probs).scatter_(1, indices, values)\n",
    "            probs /= probs.sum(dim=-1, keepdim=True)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        context = torch.cat((context, next_token), dim=1)\n",
    "\n",
    "    return decode(context[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15e9aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Печать результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d8a877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сгенерированный текст\n",
      "Я бы смог завоевать весь мир одной рукой, если бы ты торую. Очни и ты бы дер. взгдала всьшай всвное всегда всела всеты всешь всешь понае пой плеанимато. это. В безнолезношемачиной льшьной бениматьносуе и ствимать ре мадатро. Сльнельноматьне.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСгенерированный текст\")\n",
    "print(generate(model, start_text=\"Я бы смог завоевать весь мир одной рукой,\", max_new_tokens=200, temperature=0.7, top_k=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
