{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "import nltk\n",
    "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "try:\n",
    "    nltk_stopwords.words(\"russian\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_text:  ['смочь', 'завоевать', 'весь', 'мир', 'рука', 'держать', 'второй', 'очнуться', 'взглянуть', 'реальность', 'весь', 'идти', 'план', 'долгий', 'жить', 'большой', 'начинать', 'понимать', 'это', 'бесполезный']\n"
     ]
    }
   ],
   "source": [
    "text = \"Я бы смог завоевать весь мир одной рукой, если бы ты держала вторую. Очнись и взгляни на реальность,не всегда все идет по плану. Чем дольше ты живешь тем больше начинаешь понимать это. В этой бесполезной реальности существует лишь боль,ненависть и страдание. Слушай внимательно.....В этом мире везде где есть свет,всегда есть и тень. Пока в этом мире есть победители будут и проигравшие а эгоистичное желание мира всегда ведет к войне. Из желания защитить любимых людей-рождается ненависть.\"\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "embedding = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(embedding)\n",
    "\n",
    "russian_stopwords = set(nltk_stopwords.words(\"russian\"))\n",
    "\n",
    "# делим текст на параграфы\n",
    "def split_text_to_sentences(text: str) -> List[str]:\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "\n",
    "def group_sentences(sentences: List[str], group_size: int = 200) -> List[str]:\n",
    "    return [\" \".join(sentences[i:i + group_size]) for i in range(0, len(sentences), group_size)]\n",
    "\n",
    "\n",
    "def preprocess_text(paragraphs: List[str]) -> List[List[str]]:\n",
    "    result = []\n",
    "    for paragraph in paragraphs:\n",
    "        doc = Doc(paragraph)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            if token.pos in (\"PUNCT\", \"NUM\"):\n",
    "                continue\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemma = token.lemma.lower()\n",
    "            if lemma not in russian_stopwords:\n",
    "                tokens.append(lemma)\n",
    "        result.append(tokens)\n",
    "    return result\n",
    "\n",
    "print(\"preprocessed_text: \", preprocessed_paragraphs[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(docs: List[List[str]]) -> List[str]:\n",
    "    return sorted(set(token for doc in docs for token in doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words\n",
      "matrix:  [1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# bag_of_words\n",
    "\n",
    "\n",
    "def bag_of_words(text: list[str]) -> list[int]:\n",
    "    vocabulary = get_vocabulary(text)\n",
    "\n",
    "    bow_matrix = []\n",
    "    for tokens in text:\n",
    "        row = [0] * len(vocabulary)\n",
    "        for token in tokens:\n",
    "            if token in vocabulary:\n",
    "                j = vocabulary.index(token)\n",
    "                row[j] += 1\n",
    "        bow_matrix.append(row)\n",
    "\n",
    "    return bow_matrix\n",
    "\n",
    "\n",
    "matrix = bag_of_words(preprocessed_paragraphs[:10])\n",
    "print(\"Bag of Words\")\n",
    "print(\"matrix: \", matrix[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Словарь:\n",
      "['бесполезный', 'боль', 'большой', 'везде', 'вести', 'весь', 'взглянуть', 'внимательно', 'война', 'второй', 'держать', 'долгий', 'желание', 'жить', 'завоевать', 'защитить', 'идти', 'лишь', 'любимый', 'людей-рождаться']\n",
      "Матрица TF-IDF:\n",
      "[-0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.030136833937388925, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.030136833937388925, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463, -0.015068416968694463]\n"
     ]
    }
   ],
   "source": [
    "# tf_idf\n",
    "\n",
    "def compute_df(\n",
    "    vocabulary: list[str],\n",
    "    docs: list[list],\n",
    ") -> list[int]:\n",
    "    df: list[int] = [0] * len(vocabulary)\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        for tokens in docs:\n",
    "            if word in tokens:\n",
    "                df[i] += 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_idf(\n",
    "    df: list[int],\n",
    "    N: int,\n",
    ") -> list[float]:\n",
    "    idf: list[float] = [math.log(N / (1 + df_val)) for df_val in df]\n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_tf(\n",
    "    doc: list[str],\n",
    "    vocabulary: list[str],\n",
    ") -> list[float]:\n",
    "    total_words: int = len(doc)\n",
    "    word_counts: dict[str, int] = {}\n",
    "    for token in doc:\n",
    "        word_counts[token] = word_counts.get(token, 0) + 1\n",
    "\n",
    "    tf: list[float] = []\n",
    "    for word in vocabulary:\n",
    "        count: int = word_counts.get(word, 0)\n",
    "        tf_val: float = count / total_words if total_words > 0 else 0\n",
    "        tf.append(tf_val)\n",
    "    return tf\n",
    "\n",
    "\n",
    "def compute_tf_idf_for_doc(\n",
    "    doc: list[str],\n",
    "    vocabulary: list[str],\n",
    "    idf: list[float],\n",
    ") -> list[float]:\n",
    "    tf: list[float] = compute_tf(doc, vocabulary)\n",
    "    tf_idf: list[float] = [tf_val * idf_val for tf_val, idf_val in zip(tf, idf)]\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def tf_idf(docs: list[str]) -> tuple:\n",
    "    vocabulary = get_vocabulary(docs)\n",
    "    N = len(docs)\n",
    "    df = compute_df(vocabulary, docs)\n",
    "    idf = compute_idf(df, N)\n",
    "\n",
    "    tfidf_matrix: list[list] = []\n",
    "    for doc in docs:\n",
    "        tfidf_vector: list[float] = compute_tf_idf_for_doc(doc, vocabulary, idf)\n",
    "        tfidf_matrix.append(tfidf_vector)\n",
    "\n",
    "    return vocabulary, tfidf_matrix\n",
    "\n",
    "\n",
    "vocab_tfidf, tfidf_matrix = tf_idf(preprocessed_paragraphs)\n",
    "print(\"TF-IDF Словарь:\")\n",
    "print(vocab_tfidf[:20])\n",
    "print(\"Матрица TF-IDF:\")\n",
    "print(tfidf_matrix[0][:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
